{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffbbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db450442",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da15572",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype= torch.int32)     # 26 aplhabets + 1 special char ('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef541dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))       \n",
    "# taking whole dataset and passing them as single string, then constrainig them with set, which\n",
    "# throws out the duplicate, as a result we will get the set as string of 26 lower case alphabets\n",
    "\n",
    "# the charaters obtaiend from zip will be strings\n",
    "# therefore we need look up table from characetrs to integers\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}   \n",
    "# enumerate will index the characters, hence we are doing one to one mapping of the characters\n",
    "# This is 's' to 'i' mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfeb2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add the special character as well ('.')\n",
    "stoi['.'] = 0\n",
    "# lets create 'i' to 's' mapping for  better representation of matrix\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e624921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.']+list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ed2bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a prob distribution \n",
    "p = N[0].float()\n",
    "p = p/p.sum()  # normalize\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples =1, replacement = True, generator = g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac9e8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets normalize the whole distribution matrix in one go\n",
    "#'P = N.float()'\n",
    "P = (N+1).float()      # adding \"+1\" as a part of smoothing process\n",
    "P /= P.sum(1, keepdim = True)\n",
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b428f0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-559951.5625)\n"
     ]
    }
   ],
   "source": [
    "# lets analyse the prob assigned to the bigrams \n",
    "# using likelihood concept to assess the prob of given bigrams\n",
    "# since taking prob product of small values will become even more smaller\n",
    "# therefore make use log prb\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob =  P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1 \n",
    "        #print(f'{ch1}{ch2}: {logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa1b93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll = tensor(559951.5625)\n",
      "Normalized_nll =2.4543561935424805\n"
     ]
    }
   ],
   "source": [
    "# our aim is to get prob= 1, for every bigram,such that logprob will be zero for the bigram combo\n",
    "# and so will be log_likelihood\n",
    "# using loss function, we focus on reducing the error therefore we need to flip the expression\n",
    "# use negative log_likelihood\n",
    "nll = -log_likelihood\n",
    "print(f'{nll = }')\n",
    "average_nll = nll/n\n",
    "print(f'Normalized_nll ={average_nll}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a026ab",
   "metadata": {},
   "source": [
    "### Lets build a NN with the same concept of Bigram based analysis of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf588d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a training set of bigrams (x, y)\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:   # just for first word \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xs, ys\n",
    "# We will see that, even first word \"emma\" has 5 training example for bigram NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d149230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "xenc.shape\n",
    "# therefore we encoded the five examples coming from first word.\n",
    "#input layer,with 27 neurons represents the one_hot encoding featuring system to describe bigram.\n",
    "# since there 5 examples, hence the whole x matrix becomes of dimension 5x27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db201b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "         -2.9643e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01,\n",
       "         -4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
       "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01,\n",
       "          9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0730e-02,\n",
       "          2.4968e+00,  2.4448e+00],\n",
       "        [ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
       "          4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
       "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
       "          5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
       "         -2.1259e+00,  9.6041e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [-6.7006e-01, -1.2199e+00,  3.0314e-01, -1.0725e+00,  7.2762e-01,\n",
       "          5.1114e-02,  1.3095e+00, -8.0220e-01, -8.5042e-01, -1.8068e+00,\n",
       "          1.2523e+00, -1.2256e+00,  1.2165e+00, -9.6478e-01, -2.3211e-01,\n",
       "         -3.4762e-01,  3.3244e-01, -1.3263e+00,  1.1224e+00,  5.9641e-01,\n",
       "          4.5846e-01,  5.4011e-02, -1.7400e+00,  1.1560e-01,  8.0319e-01,\n",
       "          5.4108e-01, -1.1646e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a input layer of 27 neurons\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator = g)\n",
    "xenc @ W      # 1st hidden layer derived from the input layer vetor mult. with weight matrix\n",
    "# this hidden layer also contains 27 neurons\n",
    "# vectorized implementation of example training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25126dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc@W).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d3c4b",
   "metadata": {},
   "source": [
    "###  SOFTMAX Formulation (exponentiation of elements and normalization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b076537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0000), torch.Size([5, 27]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the activation values of neurons in hidden layers, are all sorts of \n",
    "# positive and negative logrithmic counts \n",
    "# what we need is simply counts for better interpretability, hence exponentiate them\n",
    "logits = (xenc@W)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims = True)     # normalize the count matrix\n",
    "probs[0].sum(), probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c06dca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]  #probability distri map for 1st example or bigram, wrt random generated weight matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf1028",
   "metadata": {},
   "source": [
    "##### Training every word create different number of bigrams and these number of bigrams become example that are to be trained. Consider if we had two words to train such that one word was producing 5 bigram and other one producing 3 bigrams. Then while training the data-set, we will treat all of the bigrams as individual examples (hence train them as 8 examples simply) and no need to make a distinction in between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92eca3",
   "metadata": {},
   "source": [
    "##### Further every bigram training will set first char as input and other char as the output(label).\n",
    "##### The input layer for every char involves 27 dimensional one_hot encoding of first character of bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d07c42",
   "metadata": {},
   "source": [
    "##### Another point to note here is that we have only one hidden layer with \"Softmax\" as the activation fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaeb7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets summarize what we have been doing with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bfa4605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "bigram example 1: .e(0, 5)\n",
      "input to the neural net: 0\n",
      "output label to the neural net: 5\n",
      "probability assigned by the net to the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log lieklihood: 4.399273872375488\n",
      "-------\n",
      "bigram example 2: em(5, 13)\n",
      "input to the neural net: 5\n",
      "output label to the neural net: 13\n",
      "probability assigned by the net to the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log lieklihood: 4.014570713043213\n",
      "-------\n",
      "bigram example 3: mm(13, 13)\n",
      "input to the neural net: 13\n",
      "output label to the neural net: 13\n",
      "probability assigned by the net to the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log lieklihood: 3.623408794403076\n",
      "-------\n",
      "bigram example 4: ma(13, 1)\n",
      "input to the neural net: 13\n",
      "output label to the neural net: 1\n",
      "probability assigned by the net to the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log lieklihood: 2.6080665588378906\n",
      "-------\n",
      "bigram example 5: a.(1, 0)\n",
      "input to the neural net: 1\n",
      "output label to the neural net: 0\n",
      "probability assigned by the net to the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log lieklihood: 4.201204299926758\n",
      "=======\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(xs.shape[0]):\n",
    "    # i-th bigram:\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('-------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]}({x}, {y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output label to the neural net:', y)\n",
    "    p = probs[i,y]\n",
    "    print('probability assigned by the net to the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log lieklihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print('=======')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1ea69",
   "metadata": {},
   "source": [
    "### To make the prediction better, use gradient descent and backpropogation\n",
    "                                    ## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b04577f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs ,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66afc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae300098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass \n",
    "xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "logits = xenc@W\n",
    "counts = logits.exp()\n",
    "probs = counts/ counts.sum(1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30cd871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the particular probs element that we need to optiomize are:\n",
    "'''probs[0, ys[0]], probs[0, ys[1]], probs[0, ys[2]], probs[0, ys[3]], probs[0, ys[4]]'''\n",
    "loss = -probs[torch.arange(5), ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "561525ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss with current parameters are:3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "print(f'loss with current parameters are:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "490544e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward_pass\n",
    "W.grad = None   # set to zero\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6717cd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 27]), torch.Size([27, 27]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.shape, W.shape\n",
    "# therefore every element of the \"W.grad\", is giving the gradient for the next step optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dddc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the pararmeter data with the gradient\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b0cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated loss is:3.7492127418518066\n"
     ]
    }
   ],
   "source": [
    "logits = xenc@W\n",
    "counts = logits.exp()\n",
    "probs = counts/ counts.sum(1, keepdims = True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "print(f'updated loss is:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba6a8f",
   "metadata": {},
   "source": [
    "### Lets clean and clear up the code\n",
    "                                Crystal Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aca248a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of example: 228146\n"
     ]
    }
   ],
   "source": [
    "# creating the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:   # including all the words \n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of example:', num)\n",
    "\n",
    "# Initializing the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce92e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5764\n",
      "2.5711\n",
      "2.5663\n",
      "2.5618\n",
      "2.5577\n",
      "2.5539\n",
      "2.5504\n",
      "2.5472\n",
      "2.5442\n",
      "2.5414\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(10):\n",
    "    # Forward Pass \n",
    "    xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "    logits = xenc@W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/ counts.sum(1, keepdims = True)\n",
    "    regularized_loss = (W**2).mean()\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * regularized_loss\n",
    "    print('{:.4f}'.format(loss.item()))\n",
    "    \n",
    "    # backward_pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a95c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
